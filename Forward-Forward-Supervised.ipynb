{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5eb21adf",
   "metadata": {},
   "source": [
    "## Student Identity\n",
    "Name: Arman Lotfalikhani <br>\n",
    "Student Number: 99109166"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "009fadcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5733d300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "train_set = MNIST(root='.', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_set = MNIST(root='.', train=False, download=True, transform=transforms.ToTensor())\n",
    "image_shape = train_set[0][0].shape\n",
    "input_dim = np.prod(image_shape).item()\n",
    "num_classes = len(MNIST.classes)\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5f46071",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, 50000, shuffle=True)\n",
    "test_loader = DataLoader(test_set, 10000, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a4956a",
   "metadata": {},
   "source": [
    "### Help from other sources\n",
    "The website https://medium.com/@Mosbeh_Barhoumi/forward-forward-algorithm-ac24d0d9ffd was used for getting the idea of using a norm-2 for the normalization at the beginning og each layer (Instead of batch normalization or norm-1, which give 10% accuracies). Also, shuffling the dataset labels for negative data was inspired from there (\"tf.random.shuffle(y)\").\n",
    "\n",
    "### Preliminary description \n",
    "About the loss function: In the article, it is said that we assume a sigmoid for the probability of a sample being positive,\n",
    "\n",
    "$$P(x=pos)=\\sigma(\\sum y_i^2 -\\theta) $$\n",
    "If we implement a NLL loss for the joint distribution (Similar to the derivation of the crossentroyloss) we have:\n",
    "$$ -\\frac{1}{N} \\log \\prod (P(X_i=pos)^{X_i==pos}(1-P(X=pos))^{X_i==neg})$$\n",
    "$$=\\frac{1}{N} (\\sum (X_i==pos) \\log (1+e^{\\theta-\\sum y_j^2}) + \\sum (X_i==neg) \\log(1+e^{-\\theta+\\sum y_j^2}) ) $$\n",
    "Which is the stated loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3b1dc1",
   "metadata": {},
   "source": [
    "### Code explanation: \n",
    "First, we explain a few choices in the implementation.<br>\n",
    "1: Norm-2 was used for the normalization: In fact, the important output of each layer is its goodness, which is a sum of squares. Any normalization that changes the relative sum of squares (norm-2 squared) for different datapoints (batch normalization, etc.) is bound to fail. <br>\n",
    "2: The minibatch size is the whole dataset. This code is still in the train() function: <br>\n",
    "for i,train in enumerate(train_dataloader,0): <br>\n",
    "      data=train <br>\n",
    "      \n",
    "The for statement itself is run on the CPU, and takes about 4 seconds, even though the code inside of it uses the GPU properly. As we need a lot a epochs for training, I was forced to use all samples at once, to train in a reasonable time. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2093e6e",
   "metadata": {},
   "source": [
    "### Code explanation: Part 2\n",
    "First, an FFLayer is implemented which combines a linear and relu PyTorch layers. The forward pass uses the mentioned normalization. <br>\n",
    "Then, the FFLayer is implemented with a threshold list (It must have one element fewer than the dimensions array or it would give an error). Also, as we need to train layer after layer, forward_for_layer() is used to forward the data from previously-trained layers to the current layer in the process. <br>\n",
    "At last, the predict() function is implemented in this way: As the labels are one-hot and we have to give all labels, we augment the input row and concatenate if with the identity matrix. Then, we add the goodnesses of all layers after proper forwarding and return the argmax.\n",
    "Parctical note: I wrongly assumed that the output of the last layer was enough for this prediction. As I got a low (10%) accuracy, I tested a few ways and came up with this method which works well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55fff25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "class FFLayer(nn.Module):\n",
    "    def __init__(self, epsilon, in_dim, out_dim, threshold, lr, device='cpu',dtype=torch.float32):\n",
    "        super().__init__()\n",
    "        self.layer=nn.Sequential(\n",
    "            nn.Linear(in_features=in_dim, out_features=out_dim, bias=True, device=device, dtype=dtype),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.threshold=threshold\n",
    "        self.device=device\n",
    "        self.dtype=dtype\n",
    "        self.optimizer=torch.optim.Adam(self.parameters(),lr)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_normalized = x / (x.norm(2, 1, keepdim=True) + 1e-8)\n",
    "        return self.layer(x_normalized) #self.layer(x)\n",
    "    \n",
    "    def optimizer_step(self, x_pos,x_neg):\n",
    "        pos_f=self.forward(x_pos)\n",
    "        neg_f=self.forward(x_neg)\n",
    "        goodness_pos=torch.square(pos_f).mean(1)\n",
    "        goodness_neg=torch.square(neg_f).mean(1)\n",
    "\n",
    "        loss=torch.log(1+ torch.cat([torch.exp(-goodness_pos+self.threshold),\n",
    "                                    torch.exp(goodness_neg-self.threshold)] ) ).mean()        \n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "        return loss\n",
    "        \n",
    "class FFNet():\n",
    "    def __init__(self,epsilon, dims, threshold_list, lr, device='cpu',dtype=torch.float32):\n",
    "        '''Note: threshold_list must have size L and dims must have the size L+1. L: number of layers'''\n",
    "        super().__init__()\n",
    "        self.layers=[]\n",
    "        self.device=device\n",
    "        for i in range(len(dims)-1):\n",
    "            self.layers.append(FFLayer(epsilon, in_dim=dims[i], out_dim=dims[i+1], device=device, \n",
    "                                       dtype=dtype, threshold=threshold_list[i], lr=lr))\n",
    "        print(self.layers)\n",
    "    def train(self, num_epochs, train_dataloader):\n",
    "        data=None\n",
    "        for i,train in enumerate(train_dataloader,0):\n",
    "            data=train\n",
    "        L=len(self.layers)\n",
    "        for l in range(L):\n",
    "            print('Training layer: ',l+1)\n",
    "            for epoch in range(num_epochs):\n",
    "                running_loss= 0.0\n",
    "                \n",
    "                images=data[0].to(self.device)\n",
    "                labels=data[1].to(self.device)\n",
    "                \n",
    "                images=torch.flatten(images,start_dim=1)/torch.max(images) #Scale between zero and one\n",
    "                ## If we use a randint with labels.shape, there is a higher chance (10%) each random label is actually correct.\n",
    "                indices=torch.randperm(labels.size(0))\n",
    "                false_labels=labels[indices]\n",
    "                pos_onehot=F.one_hot(labels, num_classes)\n",
    "                neg_onehot=F.one_hot(false_labels, num_classes)\n",
    "\n",
    "                pos_data=self.forward_for_layer( l,torch.hstack( (pos_onehot, images)) )\n",
    "                neg_data=self.forward_for_layer( l,torch.hstack((neg_onehot, images)) )\n",
    "\n",
    "                running_loss+= self.layers[l].optimizer_step(pos_data,neg_data)\n",
    "\n",
    "                running_loss_mean=running_loss\n",
    "                if (epoch+1)%5==0:\n",
    "                    print(\"Epoch: %i Running loss: %f\"%(epoch, running_loss_mean))\n",
    "                    \n",
    "    def forward_for_layer(self,i,x): #Forwards the data for layer i in the training process\n",
    "        y=x.clone()\n",
    "        for j in range(i):\n",
    "            y=self.layers[j].forward(y)\n",
    "            \n",
    "        return y.detach()\n",
    "    def predict(self,x_row):\n",
    "        xhat=x_row.clone().repeat(num_classes,1)\n",
    "        one_hot_labels=torch.eye(num_classes, device=self.device)\n",
    "        data=torch.hstack( (one_hot_labels, xhat))\n",
    "        predicted_goodness=0\n",
    "        for l in range(len(self.layers)):\n",
    "            data=self.forward_for_layer(l,data)\n",
    "            predicted_goodness+=torch.square(data).mean(1)\n",
    "        \n",
    "        prediction=predicted_goodness.argmax()\n",
    "        return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8076d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FFLayer(\n",
      "  (layer): Sequential(\n",
      "    (0): Linear(in_features=794, out_features=400, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "), FFLayer(\n",
      "  (layer): Sequential(\n",
      "    (0): Linear(in_features=400, out_features=300, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net=FFNet(epsilon=1e-2,dims=[794,400,300],device=device,dtype=torch.float32, threshold_list=[2, 3], lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9968a0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training layer:  1\n",
      "Epoch: 4 Running loss: 1.065203\n",
      "Epoch: 9 Running loss: 0.833339\n",
      "Epoch: 14 Running loss: 0.751228\n",
      "Epoch: 19 Running loss: 0.756918\n",
      "Epoch: 24 Running loss: 0.717786\n",
      "Epoch: 29 Running loss: 0.730019\n",
      "Epoch: 34 Running loss: 0.711048\n",
      "Epoch: 39 Running loss: 0.715878\n",
      "Epoch: 44 Running loss: 0.710003\n",
      "Epoch: 49 Running loss: 0.710394\n",
      "Epoch: 54 Running loss: 0.708414\n",
      "Epoch: 59 Running loss: 0.707977\n",
      "Epoch: 64 Running loss: 0.707069\n",
      "Epoch: 69 Running loss: 0.706748\n",
      "Epoch: 74 Running loss: 0.706177\n",
      "Epoch: 79 Running loss: 0.705837\n",
      "Epoch: 84 Running loss: 0.705371\n",
      "Epoch: 89 Running loss: 0.705081\n",
      "Epoch: 94 Running loss: 0.704676\n",
      "Epoch: 99 Running loss: 0.704380\n",
      "Epoch: 104 Running loss: 0.704085\n",
      "Epoch: 109 Running loss: 0.703734\n",
      "Epoch: 114 Running loss: 0.703441\n",
      "Epoch: 119 Running loss: 0.703175\n",
      "Epoch: 124 Running loss: 0.702909\n",
      "Epoch: 129 Running loss: 0.702595\n",
      "Epoch: 134 Running loss: 0.702268\n",
      "Epoch: 139 Running loss: 0.701989\n",
      "Epoch: 144 Running loss: 0.701697\n",
      "Epoch: 149 Running loss: 0.701350\n",
      "Epoch: 154 Running loss: 0.701123\n",
      "Epoch: 159 Running loss: 0.700761\n",
      "Epoch: 164 Running loss: 0.700418\n",
      "Epoch: 169 Running loss: 0.700161\n",
      "Epoch: 174 Running loss: 0.699863\n",
      "Epoch: 179 Running loss: 0.699511\n",
      "Epoch: 184 Running loss: 0.699218\n",
      "Epoch: 189 Running loss: 0.698788\n",
      "Epoch: 194 Running loss: 0.698534\n",
      "Epoch: 199 Running loss: 0.698194\n",
      "Epoch: 204 Running loss: 0.697821\n",
      "Epoch: 209 Running loss: 0.697442\n",
      "Epoch: 214 Running loss: 0.697056\n",
      "Epoch: 219 Running loss: 0.696471\n",
      "Epoch: 224 Running loss: 0.696012\n",
      "Epoch: 229 Running loss: 0.695674\n",
      "Epoch: 234 Running loss: 0.695183\n",
      "Epoch: 239 Running loss: 0.694731\n",
      "Epoch: 244 Running loss: 0.694121\n",
      "Epoch: 249 Running loss: 0.693564\n",
      "Epoch: 254 Running loss: 0.692838\n",
      "Epoch: 259 Running loss: 0.692339\n",
      "Epoch: 264 Running loss: 0.691630\n",
      "Epoch: 269 Running loss: 0.691055\n",
      "Epoch: 274 Running loss: 0.690209\n",
      "Epoch: 279 Running loss: 0.689594\n",
      "Epoch: 284 Running loss: 0.688719\n",
      "Epoch: 289 Running loss: 0.687994\n",
      "Epoch: 294 Running loss: 0.687115\n",
      "Epoch: 299 Running loss: 0.686075\n",
      "Epoch: 304 Running loss: 0.685178\n",
      "Epoch: 309 Running loss: 0.684235\n",
      "Epoch: 314 Running loss: 0.682879\n",
      "Epoch: 319 Running loss: 0.681762\n",
      "Epoch: 324 Running loss: 0.680648\n",
      "Epoch: 329 Running loss: 0.679276\n",
      "Epoch: 334 Running loss: 0.677894\n",
      "Epoch: 339 Running loss: 0.676539\n",
      "Epoch: 344 Running loss: 0.675415\n",
      "Epoch: 349 Running loss: 0.673809\n",
      "Epoch: 354 Running loss: 0.672173\n",
      "Epoch: 359 Running loss: 0.670351\n",
      "Epoch: 364 Running loss: 0.668229\n",
      "Epoch: 369 Running loss: 0.666989\n",
      "Epoch: 374 Running loss: 0.664882\n",
      "Epoch: 379 Running loss: 0.663071\n",
      "Epoch: 384 Running loss: 0.660995\n",
      "Epoch: 389 Running loss: 0.659616\n",
      "Epoch: 394 Running loss: 0.656963\n",
      "Epoch: 399 Running loss: 0.655113\n",
      "Epoch: 404 Running loss: 0.653415\n",
      "Epoch: 409 Running loss: 0.651251\n",
      "Epoch: 414 Running loss: 0.648906\n",
      "Epoch: 419 Running loss: 0.645919\n",
      "Epoch: 424 Running loss: 0.645057\n",
      "Epoch: 429 Running loss: 0.642827\n",
      "Epoch: 434 Running loss: 0.639820\n",
      "Epoch: 439 Running loss: 0.638894\n",
      "Epoch: 444 Running loss: 0.637413\n",
      "Epoch: 449 Running loss: 0.635068\n",
      "Epoch: 454 Running loss: 0.632975\n",
      "Epoch: 459 Running loss: 0.630001\n",
      "Epoch: 464 Running loss: 0.627698\n",
      "Epoch: 469 Running loss: 0.626423\n",
      "Epoch: 474 Running loss: 0.623418\n",
      "Epoch: 479 Running loss: 0.622334\n",
      "Epoch: 484 Running loss: 0.619480\n",
      "Epoch: 489 Running loss: 0.618481\n",
      "Epoch: 494 Running loss: 0.617518\n",
      "Epoch: 499 Running loss: 0.613054\n",
      "Epoch: 504 Running loss: 0.611600\n",
      "Epoch: 509 Running loss: 0.609627\n",
      "Epoch: 514 Running loss: 0.608861\n",
      "Epoch: 519 Running loss: 0.607234\n",
      "Epoch: 524 Running loss: 0.604478\n",
      "Epoch: 529 Running loss: 0.601482\n",
      "Epoch: 534 Running loss: 0.600976\n",
      "Epoch: 539 Running loss: 0.599457\n",
      "Epoch: 544 Running loss: 0.597850\n",
      "Epoch: 549 Running loss: 0.594943\n",
      "Epoch: 554 Running loss: 0.592867\n",
      "Epoch: 559 Running loss: 0.589766\n",
      "Epoch: 564 Running loss: 0.590693\n",
      "Epoch: 569 Running loss: 0.590788\n",
      "Epoch: 574 Running loss: 0.585544\n",
      "Epoch: 579 Running loss: 0.584287\n",
      "Epoch: 584 Running loss: 0.583173\n",
      "Epoch: 589 Running loss: 0.580399\n",
      "Epoch: 594 Running loss: 0.580174\n",
      "Epoch: 599 Running loss: 0.579348\n",
      "Epoch: 604 Running loss: 0.575470\n",
      "Epoch: 609 Running loss: 0.573028\n",
      "Epoch: 614 Running loss: 0.572527\n",
      "Epoch: 619 Running loss: 0.573326\n",
      "Epoch: 624 Running loss: 0.570350\n",
      "Epoch: 629 Running loss: 0.569033\n",
      "Epoch: 634 Running loss: 0.565032\n",
      "Epoch: 639 Running loss: 0.566216\n",
      "Epoch: 644 Running loss: 0.563520\n",
      "Epoch: 649 Running loss: 0.562439\n",
      "Epoch: 654 Running loss: 0.559423\n",
      "Epoch: 659 Running loss: 0.559630\n",
      "Epoch: 664 Running loss: 0.557359\n",
      "Epoch: 669 Running loss: 0.553625\n",
      "Epoch: 674 Running loss: 0.555921\n",
      "Epoch: 679 Running loss: 0.552397\n",
      "Epoch: 684 Running loss: 0.554115\n",
      "Epoch: 689 Running loss: 0.549633\n",
      "Epoch: 694 Running loss: 0.549036\n",
      "Epoch: 699 Running loss: 0.544867\n",
      "Epoch: 704 Running loss: 0.547176\n",
      "Epoch: 709 Running loss: 0.541153\n",
      "Epoch: 714 Running loss: 0.544649\n",
      "Epoch: 719 Running loss: 0.541801\n",
      "Epoch: 724 Running loss: 0.540593\n",
      "Epoch: 729 Running loss: 0.540830\n",
      "Epoch: 734 Running loss: 0.537005\n",
      "Epoch: 739 Running loss: 0.537459\n",
      "Epoch: 744 Running loss: 0.535633\n",
      "Epoch: 749 Running loss: 0.533168\n",
      "Epoch: 754 Running loss: 0.534247\n",
      "Epoch: 759 Running loss: 0.534072\n",
      "Epoch: 764 Running loss: 0.530825\n",
      "Epoch: 769 Running loss: 0.527847\n",
      "Epoch: 774 Running loss: 0.529375\n",
      "Epoch: 779 Running loss: 0.526867\n",
      "Epoch: 784 Running loss: 0.524676\n",
      "Epoch: 789 Running loss: 0.523378\n",
      "Epoch: 794 Running loss: 0.525208\n",
      "Epoch: 799 Running loss: 0.520554\n",
      "Epoch: 804 Running loss: 0.520972\n",
      "Epoch: 809 Running loss: 0.520146\n",
      "Epoch: 814 Running loss: 0.520907\n",
      "Epoch: 819 Running loss: 0.517225\n",
      "Epoch: 824 Running loss: 0.517774\n",
      "Epoch: 829 Running loss: 0.516444\n",
      "Epoch: 834 Running loss: 0.516651\n",
      "Epoch: 839 Running loss: 0.513283\n",
      "Epoch: 844 Running loss: 0.512743\n",
      "Epoch: 849 Running loss: 0.514420\n",
      "Epoch: 854 Running loss: 0.510871\n",
      "Epoch: 859 Running loss: 0.508775\n",
      "Epoch: 864 Running loss: 0.508704\n",
      "Epoch: 869 Running loss: 0.509459\n",
      "Epoch: 874 Running loss: 0.506014\n",
      "Epoch: 879 Running loss: 0.506012\n",
      "Epoch: 884 Running loss: 0.505704\n",
      "Epoch: 889 Running loss: 0.505168\n",
      "Epoch: 894 Running loss: 0.501954\n",
      "Epoch: 899 Running loss: 0.502258\n",
      "Epoch: 904 Running loss: 0.497912\n",
      "Epoch: 909 Running loss: 0.499379\n",
      "Epoch: 914 Running loss: 0.501392\n",
      "Epoch: 919 Running loss: 0.497465\n",
      "Epoch: 924 Running loss: 0.495365\n",
      "Epoch: 929 Running loss: 0.494952\n",
      "Epoch: 934 Running loss: 0.498394\n",
      "Epoch: 939 Running loss: 0.496467\n",
      "Epoch: 944 Running loss: 0.495587\n",
      "Epoch: 949 Running loss: 0.493967\n",
      "Epoch: 954 Running loss: 0.494209\n",
      "Epoch: 959 Running loss: 0.492939\n",
      "Epoch: 964 Running loss: 0.489889\n",
      "Epoch: 969 Running loss: 0.488991\n",
      "Epoch: 974 Running loss: 0.492211\n",
      "Epoch: 979 Running loss: 0.488597\n",
      "Epoch: 984 Running loss: 0.482393\n",
      "Epoch: 989 Running loss: 0.487608\n",
      "Epoch: 994 Running loss: 0.486619\n",
      "Epoch: 999 Running loss: 0.484930\n",
      "Epoch: 1004 Running loss: 0.482027\n",
      "Epoch: 1009 Running loss: 0.486376\n",
      "Epoch: 1014 Running loss: 0.482976\n",
      "Epoch: 1019 Running loss: 0.483450\n",
      "Epoch: 1024 Running loss: 0.481959\n",
      "Epoch: 1029 Running loss: 0.479467\n",
      "Epoch: 1034 Running loss: 0.479354\n",
      "Epoch: 1039 Running loss: 0.478894\n",
      "Epoch: 1044 Running loss: 0.480151\n",
      "Epoch: 1049 Running loss: 0.475334\n",
      "Epoch: 1054 Running loss: 0.476042\n",
      "Epoch: 1059 Running loss: 0.474399\n",
      "Epoch: 1064 Running loss: 0.479138\n",
      "Epoch: 1069 Running loss: 0.473669\n",
      "Epoch: 1074 Running loss: 0.475100\n",
      "Epoch: 1079 Running loss: 0.477783\n",
      "Epoch: 1084 Running loss: 0.471831\n",
      "Epoch: 1089 Running loss: 0.471817\n",
      "Epoch: 1094 Running loss: 0.470161\n",
      "Epoch: 1099 Running loss: 0.468424\n",
      "Epoch: 1104 Running loss: 0.470544\n",
      "Epoch: 1109 Running loss: 0.468594\n",
      "Epoch: 1114 Running loss: 0.467718\n",
      "Epoch: 1119 Running loss: 0.469655\n",
      "Epoch: 1124 Running loss: 0.466572\n",
      "Epoch: 1129 Running loss: 0.466133\n",
      "Epoch: 1134 Running loss: 0.464297\n",
      "Epoch: 1139 Running loss: 0.465279\n",
      "Epoch: 1144 Running loss: 0.464289\n",
      "Epoch: 1149 Running loss: 0.466418\n",
      "Epoch: 1154 Running loss: 0.463979\n",
      "Epoch: 1159 Running loss: 0.463814\n",
      "Epoch: 1164 Running loss: 0.462983\n",
      "Epoch: 1169 Running loss: 0.461729\n",
      "Epoch: 1174 Running loss: 0.463170\n",
      "Epoch: 1179 Running loss: 0.460159\n",
      "Epoch: 1184 Running loss: 0.460702\n",
      "Epoch: 1189 Running loss: 0.459401\n",
      "Epoch: 1194 Running loss: 0.458023\n",
      "Epoch: 1199 Running loss: 0.457823\n",
      "Training layer:  2\n",
      "Epoch: 4 Running loss: 1.472946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 Running loss: 1.145760\n",
      "Epoch: 14 Running loss: 0.719579\n",
      "Epoch: 19 Running loss: 0.804383\n",
      "Epoch: 24 Running loss: 0.708605\n",
      "Epoch: 29 Running loss: 0.689947\n",
      "Epoch: 34 Running loss: 0.703388\n",
      "Epoch: 39 Running loss: 0.674148\n",
      "Epoch: 44 Running loss: 0.680155\n",
      "Epoch: 49 Running loss: 0.671199\n",
      "Epoch: 54 Running loss: 0.672982\n",
      "Epoch: 59 Running loss: 0.671649\n",
      "Epoch: 64 Running loss: 0.669064\n",
      "Epoch: 69 Running loss: 0.667609\n",
      "Epoch: 74 Running loss: 0.666587\n",
      "Epoch: 79 Running loss: 0.667072\n",
      "Epoch: 84 Running loss: 0.666671\n",
      "Epoch: 89 Running loss: 0.667287\n",
      "Epoch: 94 Running loss: 0.664391\n",
      "Epoch: 99 Running loss: 0.665163\n",
      "Epoch: 104 Running loss: 0.665489\n",
      "Epoch: 109 Running loss: 0.665985\n",
      "Epoch: 114 Running loss: 0.664908\n",
      "Epoch: 119 Running loss: 0.662802\n",
      "Epoch: 124 Running loss: 0.663521\n",
      "Epoch: 129 Running loss: 0.662440\n",
      "Epoch: 134 Running loss: 0.664433\n",
      "Epoch: 139 Running loss: 0.660816\n",
      "Epoch: 144 Running loss: 0.661320\n",
      "Epoch: 149 Running loss: 0.661218\n",
      "Epoch: 154 Running loss: 0.660362\n",
      "Epoch: 159 Running loss: 0.659836\n",
      "Epoch: 164 Running loss: 0.657533\n",
      "Epoch: 169 Running loss: 0.657801\n",
      "Epoch: 174 Running loss: 0.656979\n",
      "Epoch: 179 Running loss: 0.654859\n",
      "Epoch: 184 Running loss: 0.652025\n",
      "Epoch: 189 Running loss: 0.652154\n",
      "Epoch: 194 Running loss: 0.649953\n",
      "Epoch: 199 Running loss: 0.645870\n",
      "Epoch: 204 Running loss: 0.646234\n",
      "Epoch: 209 Running loss: 0.643169\n",
      "Epoch: 214 Running loss: 0.641749\n",
      "Epoch: 219 Running loss: 0.637210\n",
      "Epoch: 224 Running loss: 0.636025\n",
      "Epoch: 229 Running loss: 0.633583\n",
      "Epoch: 234 Running loss: 0.631614\n",
      "Epoch: 239 Running loss: 0.629831\n",
      "Epoch: 244 Running loss: 0.624598\n",
      "Epoch: 249 Running loss: 0.619795\n",
      "Epoch: 254 Running loss: 0.619099\n",
      "Epoch: 259 Running loss: 0.616364\n",
      "Epoch: 264 Running loss: 0.611997\n",
      "Epoch: 269 Running loss: 0.612495\n",
      "Epoch: 274 Running loss: 0.607893\n",
      "Epoch: 279 Running loss: 0.604483\n",
      "Epoch: 284 Running loss: 0.603424\n",
      "Epoch: 289 Running loss: 0.600218\n",
      "Epoch: 294 Running loss: 0.599115\n",
      "Epoch: 299 Running loss: 0.594717\n",
      "Epoch: 304 Running loss: 0.593148\n",
      "Epoch: 309 Running loss: 0.588509\n",
      "Epoch: 314 Running loss: 0.589037\n",
      "Epoch: 319 Running loss: 0.585349\n",
      "Epoch: 324 Running loss: 0.583705\n",
      "Epoch: 329 Running loss: 0.580651\n",
      "Epoch: 334 Running loss: 0.580964\n",
      "Epoch: 339 Running loss: 0.577477\n",
      "Epoch: 344 Running loss: 0.574767\n",
      "Epoch: 349 Running loss: 0.573760\n",
      "Epoch: 354 Running loss: 0.572708\n",
      "Epoch: 359 Running loss: 0.568581\n",
      "Epoch: 364 Running loss: 0.567613\n",
      "Epoch: 369 Running loss: 0.565752\n",
      "Epoch: 374 Running loss: 0.562337\n",
      "Epoch: 379 Running loss: 0.562037\n",
      "Epoch: 384 Running loss: 0.561279\n",
      "Epoch: 389 Running loss: 0.559279\n",
      "Epoch: 394 Running loss: 0.555166\n",
      "Epoch: 399 Running loss: 0.554989\n",
      "Epoch: 404 Running loss: 0.551164\n",
      "Epoch: 409 Running loss: 0.554913\n",
      "Epoch: 414 Running loss: 0.548225\n",
      "Epoch: 419 Running loss: 0.547622\n",
      "Epoch: 424 Running loss: 0.547988\n",
      "Epoch: 429 Running loss: 0.544569\n",
      "Epoch: 434 Running loss: 0.546801\n",
      "Epoch: 439 Running loss: 0.542318\n",
      "Epoch: 444 Running loss: 0.539069\n",
      "Epoch: 449 Running loss: 0.541454\n",
      "Epoch: 454 Running loss: 0.539374\n",
      "Epoch: 459 Running loss: 0.538792\n",
      "Epoch: 464 Running loss: 0.536839\n",
      "Epoch: 469 Running loss: 0.536056\n",
      "Epoch: 474 Running loss: 0.535582\n",
      "Epoch: 479 Running loss: 0.535853\n",
      "Epoch: 484 Running loss: 0.532968\n",
      "Epoch: 489 Running loss: 0.529810\n",
      "Epoch: 494 Running loss: 0.531203\n",
      "Epoch: 499 Running loss: 0.530578\n",
      "Epoch: 504 Running loss: 0.525774\n",
      "Epoch: 509 Running loss: 0.527333\n",
      "Epoch: 514 Running loss: 0.526101\n",
      "Epoch: 519 Running loss: 0.523775\n",
      "Epoch: 524 Running loss: 0.524013\n",
      "Epoch: 529 Running loss: 0.523764\n",
      "Epoch: 534 Running loss: 0.521627\n",
      "Epoch: 539 Running loss: 0.518126\n",
      "Epoch: 544 Running loss: 0.521537\n",
      "Epoch: 549 Running loss: 0.520680\n",
      "Epoch: 554 Running loss: 0.521191\n",
      "Epoch: 559 Running loss: 0.516116\n",
      "Epoch: 564 Running loss: 0.514392\n",
      "Epoch: 569 Running loss: 0.515824\n",
      "Epoch: 574 Running loss: 0.515738\n",
      "Epoch: 579 Running loss: 0.513668\n",
      "Epoch: 584 Running loss: 0.513052\n",
      "Epoch: 589 Running loss: 0.511504\n",
      "Epoch: 594 Running loss: 0.509706\n",
      "Epoch: 599 Running loss: 0.507689\n",
      "Epoch: 604 Running loss: 0.507088\n",
      "Epoch: 609 Running loss: 0.508218\n",
      "Epoch: 614 Running loss: 0.507803\n",
      "Epoch: 619 Running loss: 0.507545\n",
      "Epoch: 624 Running loss: 0.506766\n",
      "Epoch: 629 Running loss: 0.504096\n",
      "Epoch: 634 Running loss: 0.502497\n",
      "Epoch: 639 Running loss: 0.504922\n",
      "Epoch: 644 Running loss: 0.502554\n",
      "Epoch: 649 Running loss: 0.500765\n",
      "Epoch: 654 Running loss: 0.502166\n",
      "Epoch: 659 Running loss: 0.500023\n",
      "Epoch: 664 Running loss: 0.496646\n",
      "Epoch: 669 Running loss: 0.497525\n",
      "Epoch: 674 Running loss: 0.495761\n",
      "Epoch: 679 Running loss: 0.494829\n",
      "Epoch: 684 Running loss: 0.495325\n",
      "Epoch: 689 Running loss: 0.496188\n",
      "Epoch: 694 Running loss: 0.496156\n",
      "Epoch: 699 Running loss: 0.493235\n",
      "Epoch: 704 Running loss: 0.493809\n",
      "Epoch: 709 Running loss: 0.495225\n",
      "Epoch: 714 Running loss: 0.490008\n",
      "Epoch: 719 Running loss: 0.494766\n",
      "Epoch: 724 Running loss: 0.489813\n",
      "Epoch: 729 Running loss: 0.492601\n",
      "Epoch: 734 Running loss: 0.487682\n",
      "Epoch: 739 Running loss: 0.487749\n",
      "Epoch: 744 Running loss: 0.488865\n",
      "Epoch: 749 Running loss: 0.488371\n",
      "Epoch: 754 Running loss: 0.486939\n",
      "Epoch: 759 Running loss: 0.484438\n",
      "Epoch: 764 Running loss: 0.485277\n",
      "Epoch: 769 Running loss: 0.486235\n",
      "Epoch: 774 Running loss: 0.484806\n",
      "Epoch: 779 Running loss: 0.484980\n",
      "Epoch: 784 Running loss: 0.479549\n",
      "Epoch: 789 Running loss: 0.485268\n",
      "Epoch: 794 Running loss: 0.486359\n",
      "Epoch: 799 Running loss: 0.480189\n",
      "Epoch: 804 Running loss: 0.481844\n",
      "Epoch: 809 Running loss: 0.480466\n",
      "Epoch: 814 Running loss: 0.481290\n",
      "Epoch: 819 Running loss: 0.482504\n",
      "Epoch: 824 Running loss: 0.478093\n",
      "Epoch: 829 Running loss: 0.475695\n",
      "Epoch: 834 Running loss: 0.481011\n",
      "Epoch: 839 Running loss: 0.475908\n",
      "Epoch: 844 Running loss: 0.473076\n",
      "Epoch: 849 Running loss: 0.473784\n",
      "Epoch: 854 Running loss: 0.473344\n",
      "Epoch: 859 Running loss: 0.474677\n",
      "Epoch: 864 Running loss: 0.475939\n",
      "Epoch: 869 Running loss: 0.472490\n",
      "Epoch: 874 Running loss: 0.472079\n",
      "Epoch: 879 Running loss: 0.470500\n",
      "Epoch: 884 Running loss: 0.472600\n",
      "Epoch: 889 Running loss: 0.471715\n",
      "Epoch: 894 Running loss: 0.468478\n",
      "Epoch: 899 Running loss: 0.469029\n",
      "Epoch: 904 Running loss: 0.469294\n",
      "Epoch: 909 Running loss: 0.470232\n",
      "Epoch: 914 Running loss: 0.468461\n",
      "Epoch: 919 Running loss: 0.468388\n",
      "Epoch: 924 Running loss: 0.469558\n",
      "Epoch: 929 Running loss: 0.467493\n",
      "Epoch: 934 Running loss: 0.468788\n",
      "Epoch: 939 Running loss: 0.464890\n",
      "Epoch: 944 Running loss: 0.462768\n",
      "Epoch: 949 Running loss: 0.466045\n",
      "Epoch: 954 Running loss: 0.465174\n",
      "Epoch: 959 Running loss: 0.463463\n",
      "Epoch: 964 Running loss: 0.466041\n",
      "Epoch: 969 Running loss: 0.460369\n",
      "Epoch: 974 Running loss: 0.460576\n",
      "Epoch: 979 Running loss: 0.463799\n",
      "Epoch: 984 Running loss: 0.461431\n",
      "Epoch: 989 Running loss: 0.461776\n",
      "Epoch: 994 Running loss: 0.462377\n",
      "Epoch: 999 Running loss: 0.461490\n",
      "Epoch: 1004 Running loss: 0.459120\n",
      "Epoch: 1009 Running loss: 0.461277\n",
      "Epoch: 1014 Running loss: 0.460813\n",
      "Epoch: 1019 Running loss: 0.459741\n",
      "Epoch: 1024 Running loss: 0.454869\n",
      "Epoch: 1029 Running loss: 0.456345\n",
      "Epoch: 1034 Running loss: 0.456423\n",
      "Epoch: 1039 Running loss: 0.458037\n",
      "Epoch: 1044 Running loss: 0.458812\n",
      "Epoch: 1049 Running loss: 0.459865\n",
      "Epoch: 1054 Running loss: 0.455603\n",
      "Epoch: 1059 Running loss: 0.450787\n",
      "Epoch: 1064 Running loss: 0.458104\n",
      "Epoch: 1069 Running loss: 0.452775\n",
      "Epoch: 1074 Running loss: 0.451019\n",
      "Epoch: 1079 Running loss: 0.456632\n",
      "Epoch: 1084 Running loss: 0.453552\n",
      "Epoch: 1089 Running loss: 0.453926\n",
      "Epoch: 1094 Running loss: 0.453178\n",
      "Epoch: 1099 Running loss: 0.454095\n",
      "Epoch: 1104 Running loss: 0.453059\n",
      "Epoch: 1109 Running loss: 0.456419\n",
      "Epoch: 1114 Running loss: 0.451792\n",
      "Epoch: 1119 Running loss: 0.454961\n",
      "Epoch: 1124 Running loss: 0.448515\n",
      "Epoch: 1129 Running loss: 0.446413\n",
      "Epoch: 1134 Running loss: 0.450477\n",
      "Epoch: 1139 Running loss: 0.449496\n",
      "Epoch: 1144 Running loss: 0.448362\n",
      "Epoch: 1149 Running loss: 0.447528\n",
      "Epoch: 1154 Running loss: 0.445264\n",
      "Epoch: 1159 Running loss: 0.446568\n",
      "Epoch: 1164 Running loss: 0.445314\n",
      "Epoch: 1169 Running loss: 0.445192\n",
      "Epoch: 1174 Running loss: 0.443361\n",
      "Epoch: 1179 Running loss: 0.448374\n",
      "Epoch: 1184 Running loss: 0.445410\n",
      "Epoch: 1189 Running loss: 0.443464\n",
      "Epoch: 1194 Running loss: 0.442747\n",
      "Epoch: 1199 Running loss: 0.447448\n"
     ]
    }
   ],
   "source": [
    "net.train(1200, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715b718a",
   "metadata": {},
   "source": [
    "### Model results\n",
    "In the following sections, we see the evaluation of the trained model. As the predict() function takes one row at a time to replicate the row with different (one_hot) labels to pass it to the model, we need for loops for the evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "814ae4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set accuracy is:  0.8697\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = next(iter(train_loader))\n",
    "x_train, y_train = x_train.cuda(), y_train.cuda()\n",
    "image=torch.flatten(x_train,start_dim=1)\n",
    "l=0\n",
    "N_train=x_train.size(0)\n",
    "for i in range(N_train):\n",
    "    if net.predict(image[i])!=y_train[i]:\n",
    "        l+=1\n",
    "print('Train set accuracy is: ',1-l/N_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79ff7124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy is:  0.8706\n"
     ]
    }
   ],
   "source": [
    "x_test, y_test = next(iter(test_loader))\n",
    "x_test, y_test = x_test.cuda(), y_test.cuda()\n",
    "image=torch.flatten(x_test,start_dim=1)\n",
    "l=0\n",
    "N_test=x_test.size(0)\n",
    "for i in range(N_test):\n",
    "    if net.predict(image[i])!=y_test[i]:\n",
    "        l+=1\n",
    "print('Test set accuracy is: ',1-l/N_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386e80e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
