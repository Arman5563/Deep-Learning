{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20364731-3c2d-4e6b-8550-481c745e322b",
   "metadata": {
    "id": "20364731-3c2d-4e6b-8550-481c745e322b"
   },
   "source": [
    "## Student Identity\n",
    "Name: Arman Lotfalikhani <br>\n",
    "Student Number: 99109166"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3PsOJiUlzCK_",
   "metadata": {
    "id": "3PsOJiUlzCK_"
   },
   "source": [
    "## Theoretical questions\n",
    "### A:\n",
    "In normal convolutional layers, all the samples are located in a rectangular shape, but deformable convolutions allow irregular sampling, and the samples may even be far apart\n",
    "### B:\n",
    "As the sampling is irregular, it can adapt better to transformations such as rotation, as the sampling itself can adjust to these changes.\n",
    "### C:\n",
    "Because the grids and the shifts are rectangular, when a rotation happens, the activation output of a single neuron may change substantially, but with irregular sampling, we can adjust and get a more or less equal output for the particular neoron.\n",
    "### D:\n",
    "Another normal convolutional layer with $C^\\prime = C*k^2$ is used to compute the offsets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c3b02e",
   "metadata": {
    "id": "85c3b02e"
   },
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6246005",
   "metadata": {
    "id": "b6246005"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40e10458",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "40e10458",
    "outputId": "0008ee18-cdaa-4542-b399-fe25e7b2cc0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_set = torchvision.datasets.CIFAR10(root='.', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_set = torchvision.datasets.CIFAR10(root='.', train=False, download=True, transform=transforms.ToTensor())\n",
    "trainloader = DataLoader(train_set, 64, shuffle=True)\n",
    "testloader = DataLoader(test_set, 64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ba6a266",
   "metadata": {
    "id": "0ba6a266"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1=nn.Conv2d(3,40,kernel_size=3,stride=1)\n",
    "        self.avg_p1=nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        self.conv2=nn.Conv2d(40,25,kernel_size=2,stride=1)\n",
    "        self.avg_p2=nn.AvgPool2d(kernel_size=2,stride=2)\n",
    "\n",
    "        self.lin1=nn.Linear(25*7*7,300)\n",
    "        self.lin2=nn.Linear(300,100)\n",
    "        self.lin3=nn.Linear(100,10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=F.leaky_relu(self.conv1(x))\n",
    "        x=self.avg_p1(x)\n",
    "        x=F.leaky_relu(self.conv2(x))\n",
    "        x=self.avg_p2(x)\n",
    "        x=F.leaky_relu(self.lin1(torch.flatten(x,1)))\n",
    "        x=F.leaky_relu(self.lin2(x))\n",
    "        x=self.lin3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79068600",
   "metadata": {
    "id": "79068600"
   },
   "outputs": [],
   "source": [
    "net = Net()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "loss_func=nn.CrossEntropyLoss()\n",
    "gpu_net=net.to(device)\n",
    "optimizer=torch.optim.Adam(gpu_net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbf5c8d2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fbf5c8d2",
    "outputId": "855f6d84-0ec4-4157-86c3-69d73233e42c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Running loss: 1.519513 Accuracy: 45\n",
      "Epoch: 2 Running loss: 1.162651 Accuracy: 59\n",
      "Epoch: 3 Running loss: 0.995600 Accuracy: 66\n",
      "Elapsed time:  34.957897424697876\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "epoch_nums = 3\n",
    "t=time.time()\n",
    "for epoch in range(epoch_nums):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    for data in trainloader:\n",
    "        images=data[0].to(device)\n",
    "        my_classes=data[1].to(device)\n",
    "\n",
    "        logits=gpu_net(images)\n",
    "        loss=loss_func(logits,my_classes)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        predictions=torch.argmax(gpu_net(images),1)\n",
    "        correct+=torch.sum(torch.eq(predictions,my_classes)).item()\n",
    "        total+=len(predictions)\n",
    "\n",
    "        running_loss=running_loss+loss.item()/len(trainloader)\n",
    "    print(\"Epoch: %i Running loss: %f Accuracy: %i\"%(epoch+1,running_loss,100*correct//total))\n",
    "print('Elapsed time: ',time.time()-t)\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb0f4803",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fb0f4803",
    "outputId": "cd3404dc-7d31-4001-e89e-ce10309450f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 64 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images_gpu=data[0].to(device)\n",
    "        my_classes2_gpu=data[1].to(device)\n",
    "        predictions=torch.argmax(gpu_net(images_gpu),1)\n",
    "        correct+=torch.sum(torch.eq(predictions,my_classes2_gpu)).item()\n",
    "        total+=len(predictions)\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e3599cc-6328-4cc8-826a-c71619a75b5c",
   "metadata": {
    "id": "6e3599cc-6328-4cc8-826a-c71619a75b5c"
   },
   "outputs": [],
   "source": [
    "class MyDeformConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels,\n",
    "                 kernel_size, stride=1, padding=1, bias=False):\n",
    "        super().__init__()\n",
    "        self.deformable=torchvision.ops.DeformConv2d(in_channels=in_channels,\n",
    "                                      out_channels=out_channels,\n",
    "                                      kernel_size=kernel_size,\n",
    "                                      stride=stride,\n",
    "                                      padding=padding)\n",
    "        self.normal_conv=nn.Conv2d(in_channels=in_channels,\n",
    "                                   out_channels=2*kernel_size**2,\n",
    "                                   kernel_size=kernel_size,\n",
    "                                   stride=stride,\n",
    "                                   padding=padding,\n",
    "                                   bias=True)\n",
    "    def forward(self, x):\n",
    "        offset = self.normal_conv(x)\n",
    "        out= self.deformable(x, offset, mask=None)\n",
    "        return out\n",
    "\n",
    "class Deformable_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1=MyDeformConv2d(3,40,kernel_size=3,stride=1)\n",
    "        self.max_p1=nn.MaxPool2d(kernel_size=2,stride=2)# becomes 40 15*15\n",
    "        self.conv2=MyDeformConv2d(40,25,kernel_size=2,stride=1)#becomes 20 14*14\n",
    "        self.max_p2=nn.MaxPool2d(kernel_size=2,stride=2)#becomes 20 7*7\n",
    "\n",
    "        self.lin1=nn.Linear(25*8*8,300)\n",
    "        self.lin2=nn.Linear(300,100)\n",
    "        self.lin3=nn.Linear(100,10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=F.leaky_relu(self.conv1(x))\n",
    "        x=self.max_p1(x)\n",
    "        x=F.leaky_relu(self.conv2(x))\n",
    "        x=self.max_p2(x)\n",
    "        x=F.leaky_relu(self.lin1(torch.flatten(x,1)))\n",
    "        x=F.leaky_relu(self.lin2(x))\n",
    "        x=self.lin3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f695c6-6491-48ea-97a2-892293d2b28c",
   "metadata": {},
   "source": [
    "We have tried that the deformable network be as similar as possible to the original one, to capture the differences well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c164dc30",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c164dc30",
    "outputId": "7e268d3f-5099-4e3e-ad42-05c69b110a7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Running loss: 1.530542\n",
      "Epoch: 2 Running loss: 1.164199\n",
      "Epoch: 3 Running loss: 0.988943\n",
      "Elapsed time:  81.86091017723083\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "net2= Deformable_Net()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "loss_func=nn.CrossEntropyLoss()\n",
    "gpu_net=net2.to(device)\n",
    "\n",
    "optimizer=torch.optim.Adam(gpu_net.parameters())\n",
    "epoch_nums = 3\n",
    "t=time.time()\n",
    "for epoch in range(epoch_nums):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        images=data[0].to(device)\n",
    "        my_classes=data[1].to(device)\n",
    "\n",
    "        logits=gpu_net(images)\n",
    "        loss=loss_func(logits,my_classes)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        running_loss=running_loss+loss.item()/len(trainloader)\n",
    "    print(\"Epoch: %i Running loss: %f\"%(epoch+1,running_loss))\n",
    "print('Elapsed time: ',time.time()-t)\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12612488-5030-4129-980b-0f94321c633e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "12612488-5030-4129-980b-0f94321c633e",
    "outputId": "4dce8bb4-d918-44f8-ab4a-02a86697a683"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 66 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images_gpu=data[0].to(device)\n",
    "        my_classes2_gpu=data[1].to(device)\n",
    "        predictions=torch.argmax(gpu_net(images_gpu),1)\n",
    "        correct+=torch.sum(torch.eq(predictions,my_classes2_gpu)).item()\n",
    "        total+=len(predictions)\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "YkS1BVopdt_M",
   "metadata": {
    "id": "YkS1BVopdt_M"
   },
   "outputs": [],
   "source": [
    "class Deformable_Net2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1=MyDeformConv2d(3,100,kernel_size=3,stride=1)\n",
    "        self.max_p1=nn.MaxPool2d(kernel_size=2,stride=2)# becomes 40 15*15\n",
    "        self.conv2=MyDeformConv2d(100,40,kernel_size=3,stride=1)#becomes 20 14*14\n",
    "        self.max_p2=nn.MaxPool2d(kernel_size=2,stride=2)#becomes 20 7*7\n",
    "        self.conv3=MyDeformConv2d(40,25,kernel_size=3,stride=1)\n",
    "\n",
    "        self.lin1=nn.Linear(1600,300)\n",
    "        self.lin2=nn.Linear(300,100)\n",
    "        self.lin3=nn.Linear(100,10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=F.leaky_relu(self.conv1(x))\n",
    "        x=self.max_p1(x)\n",
    "        x=F.leaky_relu(self.conv2(x))\n",
    "        x=self.max_p2(x)\n",
    "        x=F.leaky_relu(self.conv3(x))\n",
    "        x=F.leaky_relu(self.lin1(torch.flatten(x,1)))\n",
    "        x=F.leaky_relu(self.lin2(x))\n",
    "        x=self.lin3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3850676d-4e7b-493f-808f-88f409c1e720",
   "metadata": {},
   "source": [
    "Another deformable network with one more layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1w6vZcgMdqre",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1w6vZcgMdqre",
    "outputId": "55d3bfbe-281a-48f5-87f0-9bb3cad4fbb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Running loss: 1.562213 Accuracy: 44\n",
      "Epoch: 2 Running loss: 1.136853 Accuracy: 61\n",
      "Epoch: 3 Running loss: 0.963186 Accuracy: 68\n",
      "Elapsed time:  261.97031593322754\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "net2= Deformable_Net2()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "loss_func=nn.CrossEntropyLoss()\n",
    "gpu_net=net2.to(device)\n",
    "optimizer=torch.optim.Adam(gpu_net.parameters())\n",
    "epoch_nums = 3\n",
    "t=time.time()\n",
    "for epoch in range(epoch_nums):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    for data in trainloader:\n",
    "        images=data[0].to(device)\n",
    "        my_classes=data[1].to(device)\n",
    "\n",
    "        logits=gpu_net(images)\n",
    "        loss=loss_func(logits,my_classes)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        predictions=torch.argmax(gpu_net(images),1)\n",
    "        correct+=torch.sum(torch.eq(predictions,my_classes)).item()\n",
    "        total+=len(predictions)\n",
    "\n",
    "        running_loss=running_loss+loss.item()/len(trainloader)\n",
    "    print(\"Epoch: %i Running loss: %f Accuracy: %i\"%(epoch+1,running_loss,100*correct//total))\n",
    "print('Elapsed time: ',time.time()-t)\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1Te1-1pLfqhY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Te1-1pLfqhY",
    "outputId": "04de0331-8d13-482a-bc67-b732c80f89b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 66 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images_gpu=data[0].to(device)\n",
    "        my_classes2_gpu=data[1].to(device)\n",
    "        predictions=torch.argmax(gpu_net(images_gpu),1)\n",
    "        correct+=torch.sum(torch.eq(predictions,my_classes2_gpu)).item()\n",
    "        total+=len(predictions)\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17793799-0552-4730-852d-4bbc9c6a9634",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "We note that the accuracy has a slight 2% increase. In practice, both netowrks tend to overfit the dataset for larger epochs, so the difference cannot be captured in those cases. Also, as the error is $1-accuracy$, we have not printed it explicitly. The disadvantage of the deformable network is its longer training time, as it took more than twice the time for training a normal network (81 vs 35 seconds).\n",
    "Also, adding a single more layer increases the time to 262 seconds, which is much higher that the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4961afcf-6a46-4397-95d5-bc125f473024",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
